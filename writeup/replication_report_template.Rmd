---
title: "Replication of Sainburg 2020 and Combination with Smith & Lewicki 2006
"
author: "Nathan Tansey"
date: "2024-10-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

During Tim Sainburg's time at UCSD in Tim Gentner's lab, he published several papers, creating and inspiring many of the analysis techniques used to this day within animal vocalization research. Among these is the method described in "Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires", which is agnostic to specific animal vocalizations. Part of my prior research has been in adaptations of vocalizations to environmental distortion, this toolbox could allow easier comparisons of animals dealing with specific environments. Additionally, understanding and replicating the computational methods will establish another tool to use at baseline for any analysis of ongoing experimentation within the Gentner lab. Due to my computational background, and discussions with Tim Gentner, I will also add the base framework of sparse coding representations as outlined by Smith & Lewicki's "Efficient Auditory Coding", which I have used to success in past research to isolate the essential features of audio.

The first step needed will be data preparation. Due to the high availability of birdsong both online and internally in the Gentner lab, it is not necessary to use exact data from the paper to reproduce techniques or results, as the paper is designed to work beyond hyper specific vocalizations, but due to future research focusing on birds, I will still use birdsong. The first challenge for this project will be precise segmentation of birdsong into syllables. There are multiple computational methods, all of which have different benefits, it may be necessary to run multiple and segment by agreement between methods. 

From there, Sainburg 2020 extensively uses dimensionality reduction techniques, primarily UMAP, and develops methods for rating the dimensional space from the clustering that appears. A challenge here will be developing both methods of contrasting these results to a sparse space a la Smith & Lewicki, and/or fitting the sparse coder into the pipeline prior to UMAP to compare to Fig 3 in Sainburg 2020. Sainburg 2020 has many figures illustrating the technique on a variety of datasets/vocalizations. For my purposes, my plan is to restrain this to birdsong and human vocalizations to assess the clusterability differences between those with both different data and different models.  


github repo: https://github.com/nathanjtansey/sainburg2020

Sainburg 2020 link: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008228

Smith & Lewicki 2006 link: https://nature.com/articles/nature04485    


<!-- ######################################################################################## -->

## Methods

### Power Analysis

<!-- Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size. -->
Power analysis should not be necessary due to the nature of the original paper.


### Planned Sample

<!-- Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any. -->
The data required to replicate all figures and analysis done in the paper is hosted at https://zenodo.org/records/3775893#.X3YdqZNKhTY, or linked to Tim Sainburg's github. 


### Materials

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

### Procedure	

<!-- Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article. -->

General Preprocessing:
With each specific song, start by segmenting the data into specific syllables via dynamic thresholding, storing specific metadata for each syllable, collecting all syllables into a pandas df. These syllables will be converted from waveforms to Mel spectrograms, and then normalized and padded to be the same length. These are the same methods used in the original article.

Sparse Coding Preprocessing:
To learn the sparse components, different birdsongs will be imported from the Gentner lab. This will allow for separate training and testing datasets. A sklearn dictionary learner will be fitted using the training data in Mel spectrogram form on 10 components, and then fed into the sparse coder to create a common coder dictionary. The coder can be now used to convert any Mel spectrogram into sparse components. Theoretically, these sparse components should contain all of the relevant information, and none of the irrelevant information. The sparse coder can be run on the segmented data, giving sparse syllable components. These sparse syllable components can also be used to reconstruct syllable spectrograms by convolving the sparse components with the dictionary elements. Both the sparse syallable components and reconstructed syllable spectrograms will be saved for further analysis.

The initial preprocessing listed here results in 3 different representations of the same audio: standard mel spectrogram, sparse component representation, and reconstructed mel spectrograms. 


### Analysis Plan

<!-- Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.   -->

"Two methods for choosing feature-sets are commonly used by experimenters when the features underlying vocal data are unknown: (1) extract common descriptive statistics of vocalizations, sometimes called Predefined Acoustical Features (PAFs; e.g. mean fundamental frequency, syllable length, spectral entropy) and make comparisons on the basis of PAFs, or (2) make comparisons based upon time-frequency representations of the data (i.e. spectrograms) where the magnitude of each time-frequency component in the spectrogram is treated as an independent feature (or dimension) of the vocalization." (Sainburg 2020)

(1) Calculating PAFs does not make sense on the sparse component syllables separated out, so will be specifically be performed on the original Mel spectrogram vs the reconstructed Mel spectrogram. Next, "...computed both spectrographic representations of syllables as well as a set of 18 temporal, spectral, and fundamental characteristics (S2 Table) over each syllable using the BioSound python package [24]. We then projected both the spectral representation as well as the PAFs into 2D UMAP feature spaces" (Sainburg 2020). The 2 representations will be compared using silhouette scores: "To quantify the difference in how well clustered the different data representations are, we compare the silhouette score of each representation. The silhouette score is a measure of how well a dataset is clustered relative to a set of known category labels (e.g. syllable label, species identity). The silhouette score is the mean silhouette coefficient across all of the samples in a dataset, where the silhouette coefficient measures how distant each point is to points in its own category, relative to its distance from the nearest point in another category" (Sainburg 2020). Other acoustic statistics will be added to this list, the bit rate (Shannon entropy / time) is of particular interest, to test how discriminatory overall informtion is for each syllable.

(2) All 3 original representations from the preprocessing steps will then be projected into 2D UMAP feature spaces. These will also be compared to each other, and to the (1)'s analysis using PAFs via silhouette scores. 

This initial analysis is designed to test the relevance of the sparse coding framework with Tim Sainburg's analysis technique. At this point, it will likely become clear through the silhouette scores the interaction between these two different methods. If sparse coding does not fit into this framework, the ensuing analysis will be performed in a computational reproducability capacity only. If sparse coding frameworks perform similarly, and especially if it performs better, the data will continue to be applied.

Use HDBSCAN to identify specific clusters in UMAP projected space, and then plot the sequential organization of each label. Ideally this should be consistent across all songs from a given bird, and can be visualized over original songs. 


**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.

### Differences from Original Study

<!-- Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect. -->

The primary intended difference here comes from the introduction of the sparse coding element to the inputs. These are unknown whether they will impact results. Presumably, the sparse coder will use encode similar important features to the current analysis, that will lend themselves to similar results ultimately in high dimensional space.

The same initial dataset will be used, but many of the techniques past this are not consistent. In particular, the song syllable segmenter and UMAP have variance that can lead to small differences. Due to the large sample size and method of the paper, I do not expect this to impact many conclusions and results.

<!-- #################################################################################################################### -->

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


## Results


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
